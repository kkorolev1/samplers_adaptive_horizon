# Generative Flow Networks (GFN) with Trajectory Balance (TB) loss
name: gfn_non_acyclic_baseline
step_size: ${target.gfn_non_acyclic.step_size}
bwd_step_size: ${target.gfn_non_acyclic.bwd_step_size}
batch_size: ${target.all.batch_size}
iters: ${target.all.iters}
num_steps: ${target.gfn_non_acyclic.num_steps}
grad_clip: 1.
weight_decay: 0.0
loss_type: "subtb"
init_logZ: ${target.gfn_non_acyclic.init_logZ}
logZ_step_size: ${target.gfn_non_acyclic.logZ_step_size}
init_std: ${target.gfn_non_acyclic.init_std}  # for ou
init_invtemp: 1.
logr_clip: -1e5
huber_delta: 1e2
step_name: "ula"
reg_coef: ${target.gfn_non_acyclic.reg_coef}
no_term: False
eval_max_steps: 1000

# Learning rate scheduler
lr_schedule:
  type: multistep  # multistep | cosine | constant
  milestones: ${target.all.milestones}  # iteration indices at which to decay
  gamma: 0.3  # multiply LR by gamma at each milestone

defaults:
  - model: pisgrad_net

model:
  num_hid: ${target.all.num_hid}
  weight_init: 1e-8  # Initialization of the last layers' weights of the time-dependent network
  bias_init: 0.1  # Initialization of the last layers' bias of the time-dependent network
  gamma: ${target.gfn_non_acyclic.gamma}
  learn_fwd_corrections: False
  shared_model: False
  disable_clf: False

buffer:
  use: True
  max_length_in_batches: 100
  prioritize_by: "reward"  # none, reward, loss, uiw, piw
  target_ess: 0.05
  sampling_method: "systematic"  # multinomial, stratified, systematic, rank
  rank_k: 0.01  # only used if sampling_method is rank
  sample_with_replacement: True
  prefill_steps: 100  # collect `prefill_steps` batches before starting training
  bwd_to_fwd_ratio: 2  # number of bwd steps per fwd step
  update_score: False

local_search:
  use: True
  cycle: 25
  gamma: 1e-3
  num_steps: 300